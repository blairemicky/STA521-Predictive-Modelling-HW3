---
title: "HW3 Team 12"
author: "Thomas Fleming, Blaire Li, Marc Ryser, Hengqian Zhang"
date: "Due  February 10, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(foreign)
library(magrittr)
library(dplyr)
library(ggplot2)
library(xtable)
# add other libraries
```


We will explore logistic regression with the National Election Study data from Gelman & Hill (GH).  (See Chapter 4.7 for descriptions of some of the variables and 5.1 of GH for initial model fitting). 

[*The following code will read in the data and perform some filtering/recoding. Remove this text and modify the  code chunk options so that the code does not appear in the output.*]

```{r}
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/nes

nes <- read.dta("nes5200_processed_voters_realideo.dta",
                   convert.factors=F)
# Data cleaning
# remove NA's for key variables first
nes1992 = nes %>% filter(!is.na(black)) %>%
              filter(!is.na(female)) %>%
              filter(!is.na(educ1)) %>%
              filter(!is.na(age)) %>%
              filter(!is.na(state)) %>%
              filter(!is.na(income)) %>%
              filter(presvote %in% 1:2) %>% 
# limit to year 1992 to 2000 and add new variables
              filter(year == 1992) %>%
              mutate(female = gender -1,
                     black=race ==2,
                     vote = presvote ==2)
```

1. Summarize the data for 1992 noting which variables have missing data.  Which variables are categorical but are coded as numerically? 

ANSWER: In 1992, information in the form of 57 data fields was gathered from 1179 individuals (5 data fields specify survey details). Several fields have missing data values. The list below shows the names and numbers of missing entries (the data fields without missing entries are not listed). All variables are coded as numerical (except for 'black' and 'vote' which are logicals). However, most variables are categorical, with the exception of the following: the survey weights 'weight1', 'weight2', 'weight3', 'age', 'age_sq' (which is the age variable squared), 'ideo_feel', 'age_10' (=age/10) and 'age_sq_10' (=age^/10).

```{r, results='asis', echo=T}
#attach(nes1992)
a<-(apply(is.na(nes1992),2,sum))
#print(a[a>0])
print(a[a==0])
apply(nes1992,2,unique)
```

2. Fit the logistic regression to estimate the probability that an individual would vote Bush (Republican) as a function of income and provide a summary of the model.

ANSWER: First, we use the 'glm' function to perform a logistic regression of vote onto income.

```{r, }
vote.glm1 <-glm(vote ~ income, data=nes1992, family = binomial(link = "logit"))
summary(vote.glm1)
```

Based on this, we can estimate (point estimate) the probability that people of different income (1,2,3,4 or 5) vote for Bush. Below are the probabilities for increasing income levels

```{r}
inc<-1:5
w<-exp(vote.glm1$coefficients[1]+vote.glm1$coefficients[2]*inc)
v<-round(w/(1+w),3)
names(v)<-c("inc=1", "inc=2",  "inc=3", "inc=4", "inc=5")
print(v)
```


3. Obtain a point estimate and create a 95% confidence interval for the odds ratio for voting Republican for a rich person (income category 5) compared to a poor person (income category 1). *Hint this is more than a one unit change; calculate manually and then show how to modify the output from confint*. Provide a sentence interpreting the result.

ANSWER: We have that the odds ratio is equal to $e^{\beta_1 (5-1)}= e^{4\beta_1}$n (the intercept cancels out in the ratio of odds). Hence, we compute the confidence interval for $\beta_1$ and obtain the interval for the odds ratio as follows

```{r}
a<-confint(vote.glm1)
or.conf<-exp(4*a[2,])
suppressMessages(print(or.conf))
```

With 95\% confidence, the odds for a rich person (income=5) to vote Republican (relative to the odds for a poor person (inc=1) to vote Republican) are contained in the interval $[2.37, 5.78]$.


4.  Obtain fitted probabilities and 95% confidence intervals for the income categories using the `predict` function.  Use `ggplot` to recreate the plots in figure 5.1 of Gelman & Hill.  *write a general function?*

ANSWER: First, use the 'preidict' function to extract the fitted probabilities and the standard errors. Then we use normal theory to obtain the 95\% CI. 

```{r}
new <- data.frame(income = c(1,2,3,4,5))
glm_fit<-predict(vote.glm1, newdata = new,  type="response" ,se.fit = TRUE)
inc<-c(1,2,3,4,5)
f<-data.frame(Income=inc, Prob=round(glm_fit$fit,3), CI.low=round(glm_fit$fit-1.96*glm_fit$se.fit,3),CI.up=round(glm_fit$fit+1.96*glm_fit$se.fit,3))
print(xtable(f), comment=F)

```

Next, we reproduce the two plots in Figure 5.1 in Gelman and Hill. For the second plot, we consulted their code to understand what they did: simulate 20 model coefficients based on the sampling distributions and create the corresponding plots. We repeated this procedure and used ggplot2 to make the corresponding plots.

```{r, echo=T}


vote.glm1 <-glm(vote ~ income, data=nes1992, family = binomial(link = "logit"))

# Figure 5.1(a)
x<-seq(from=-4, to=10, by=.5)
M<-data.frame(xx=x, prob.smooth=invlogit(coef(vote.glm1)[1]+coef(vote.glm1)[2]*x))
p1<-ggplot(M, aes(x=xx, y=prob.smooth))
fig1<-p1+geom_line(aes(y = prob.smooth))+geom_point(data=nes1992, aes(x=income, y=1*vote),size=.25,position = position_jitter(width = 0.25, height = .02))+geom_line(data=subset(M, M$xx>=1 & M$xx<=5), aes(x=xx, y=prob.smooth), size=2)+labs(x="Income", y="Pr(Rebublican vote)")+scale_x_continuous(breaks=c(1,2,3,4,5))

# Figure 5.1(b)
sims<-sim(vote.glm1)
x<-seq(from=1, to=5, by=.05)
M<-data.frame(xx=x, prob.smooth=invlogit(coef(vote.glm1)[1]+coef(vote.glm1)[2]*x))
fig2<-ggplot(M, aes(x=xx, y=prob.smooth))+geom_line(aes(y = prob.smooth))+geom_point(data=nes1992, aes(x=income, y=1*vote),size=.25,position = position_jitter(width = 0.25, height = .02))+labs(x="Income", y="Pr(Rebublican vote)")+scale_x_continuous(breaks=c(1,2,3,4,5))
for(q in 1:20){
  M2<-data.frame(xx=x, prob.smooth=invlogit(sims@coef[q,1]+sims@coef[q,2]*x))

  fig2<-fig2+geom_line(data=M2,aes(y = prob.smooth), size=.25, linetype=3)
}

par(mfrow=c(2,2)) 
plot(fig1)

plot(fig2)

```



5.  What does the residual deviance or any diagnostic plots suggest about the model?  (provide code for p-values and output and plots) 

ANSWER: First we compute the residual deviance, defined as the change in deviance between the fitted model and the saturated model, and use the $\chi^2$ statistic to check whether the model is satisfactorily.

```{r}
print(vote.glm1$deviance)
pchisq(vote.glm1$deviance, vote.glm1$df.residual, lower.tail = F)

```

The very high residual deviance (and corresponding small p-value) implies that the model is not satisfactory. Next, we consider the summary plots

```{r}
par(mfrow=c(2,2)) 
plot(vote.glm1)

```
We need to proceed with caution in interpreting these plots. The residuals vs fitted pot does not look alarming per se, the Q-Q plot is off the line for normality, but this doesn't mean there is an issue a priori (there are not very heavy tails).  Heteroscidacity is to be expected in a logistic regression model. The leverage plot does not reveal any large leverage points, and the Pearson residuals are not excessively large. None of the points exceeds Cook's distance of 0.5 which is reassuring. 


6. Create a new data set by the filtering and mutate steps above, but now include years between 1952 and 2000.

```{r}
nes52.2000 = nes %>% filter(!is.na(black)) %>%
              filter(!is.na(female)) %>%
              filter(!is.na(educ1)) %>%
              filter(!is.na(age)) %>%
              filter(!is.na(state)) %>%
              filter(!is.na(income)) %>%
              filter(presvote %in% 1:2) %>% 
              filter(year >= 1952 & year <= 2000) %>%
              mutate(female = gender -1,
                     black=race ==2,
                     vote = presvote == 2)
```


7. Fit a separate logistic regression for each year from 1952 to 2000, using the `subset` option in `glm`,  i.e. add `subset=year==1952`.  For each find the 95% Confidence interval for the odds ratio of voting republican for rich compared to poor for each year in the data set from 1952 to 2000.

```{r}

## MDR: new version that  avoids growing arrays
yr <- unique(nes52.2000$year)
OR<-data.frame(year=yr, est=numeric(length(yr)), CI.lwr=numeric(length(yr)),CI.upr=numeric(length(yr)) )
inc.indiv<-rep(0,length(yr)) # used for question 9
intercept.store<-rep(0,length(yr)) # used for question 9
for (i in 1:length(yr)) {
rep <- glm(vote ~ income, data=nes52.2000, family=binomial(link="logit"), subset=year==yr[i])
OR$est[i]<-exp(4*rep$coefficients[2])
rep.ci <- suppressMessages(confint(rep))
OR[i,3:4] <- exp(4*rep.ci[2,])
inc.indiv[i]<-rep$coefficients[2]
intercept.store[i]<-rep$coefficients[1]
}
xtable(round(OR,2))

# old version:
# yr <- unique(year)
# intercept.store <- NULL
# inc.indiv <- NULL
# odds.ci <- NULL
# x.store <- NULL
# for (i in 1:length(yr)) {
# rep <- glm(vote ~ income, data=nes52.2000, family=binomial(link="logit"), subset=year==yr[i])
# intercept <- rep$coefficients[1]
# inc <- rep$coefficients[2]
# rep.ci <- suppressMessages(confint(rep))
# odds <- exp(4*rep.ci[2,])
# x <- model.matrix(rep)
# x.store <- rbind(x.store, x, deparse.level = 0)
# intercept.store <- rbind(intercept.store, intercept)
# inc.indiv <- rbind(inc.indiv, inc, deparse.level = 0)
# odds.ci <- rbind(odds.ci, odds, deparse.level = 0)
# }
# odds.ci

```





8.  Using `ggplot` plot the confidence intervals over time similar to the display in Figure 5.4.

```{r}
# use ggplot2
p<-ggplot(OR, aes(x=year, y=est)) +geom_point()+  geom_errorbar(aes(ymin=CI.lwr, ymax=CI.upr), width=.1)
p<-p+xlab("Year") + ylab("Odds ratio (95%-CI)")+ylim(0,10) + geom_hline(yintercept=c(1), linetype="dotted")
plot(p)

# old version
# plot(rep(yr, 2), odds.ci, xlim=c(1952,2000), ylab="Confidence Intervals", xlab="Year", pch = 20, col="firebrick")
# for (i in 1:length(yr)){
#   lines(rep(yr[i],2), c(odds.ci[i,1], odds.ci[i,2]), col="firebrick")
# }
```

9. Fit a logistic regression using income and year as a factor  with an interaction i.e. `income*factor(year)` to the data from 1952-2000.  Find the log odds ratio for income for each year by combining parameter estimates and show that these are the same as in the respective individual logistic regression models fit separately to the data for each year.

```{r}
inc.int <- rep(0,length(unique(nes52.2000$year)))
rep_interact <- glm(vote ~ income*factor(year), data = nes52.2000, family=binomial(link="logit"))
for (i in 1:length(yr)) {
if (i==1) {
  inc.i <- rep_interact$coefficients[2]
} else {
inc.i <- rep_interact$coefficients[2] + rep_interact$coefficients[i + 13]
}
inc.int[i]<-inc.i;
}
cbind(inc.int,inc.indiv)
```
As shown above, the log odds ratios for income are exactly the same generated from both processes (the individual regression coefficients were generated in #7).

10.  Create a plot of fitted probabilities and confidence intervals as in question 4, with curves for all years in the same plot.

```{r}
# Figure 5.1(a)
x<-seq(from=-4, to=10, by=.5)
M<-data.frame(xx=x, prob.smooth=invlogit(coef(rep)[1]+coef(rep)[2]*x))
p1<-ggplot(M, aes(x=xx, y=prob.smooth))
fig1<-p1+geom_line(aes(y = prob.smooth))+geom_point(data=nes52.2000, aes(x=income, y=1*vote),size=.25,position = position_jitter(width = 0.35, height = .1))+geom_line(data=subset(M, M$xx>=1 & M$xx<=5), aes(x=xx, y=prob.smooth), size=2)+labs(x="Income", y="Pr(Rebublican vote)")+scale_x_continuous(breaks=c(1,2,3,4,5))

# Figure 5.1(b)
sims<-sim(rep)
x<-seq(from=1, to=5, by=.05)
M<-data.frame(xx=x, prob.smooth=invlogit(coef(rep)[1]+coef(rep)[2]*x))
fig2<-ggplot(M, aes(x=xx, y=prob.smooth))+geom_line(aes(y = prob.smooth))+geom_point(data=nes52.2000, aes(x=income, y=1*vote),size=.25,position = position_jitter(width = 0.25, height = .1))+labs(x="Income", y="Pr(Rebublican vote)")+scale_x_continuous(breaks=c(1,2,3,4,5))
for(q in 1:20){
  M2<-data.frame(xx=x, prob.smooth=invlogit(sims@coef[q,1]+sims@coef[q,2]*x))

  fig2<-fig2+geom_line(data=M2,aes(y = prob.smooth), size=.25, linetype=3)
}

par(mfrow=c(2,2)) 
plot(fig1)

plot(fig2)
#old code:
#curve (invlogit(intercept.store[1] + inc.indiv[1]*x), 1, 5, ylim=c(0,1),
 #        xlim=c(1,5), ylab="Pr (Republican vote)", xlab="Income", lwd=1, #col=21)
#for (i in 2:length(yr)) {
#curve (invlogit(intercept.store[i] + inc.indiv[i]*x), 1, 5, ylim=c(0,1),
 #        xlim=c(1,5), ylab="Pr (Republican vote)", xlab="Income", lwd=1, #col=i+20, add=T)
#}
#  mtext ("(poor)", 1, 1.5, at=1, adj=.5)
#  mtext ("(rich)", 1, 1.5, at=5, adj=.5)
#  points (jitter (income, 1), jitter (as.numeric(vote), 1), pch=20, cex=.15, #col=alpha("hotpink", .25))
```
*Used code from Gelman & Hill Figure 5.1 as guidance.

11.  Return to the 1992 year data. Filter out rows of `nes1992` with NA's in the variables below and  recode as factors using the levels in parentheses:
    + gender (1 = "male", 2 = "female"), 
    + race (1 = "white", 2 = "black", 3 = "asian", 4 = "native american", 5 = "hispanic", 7 = "other"), 
    + education ( use `educ1` with levels 1 = "no high school", 2 = "high school graduate", 3 = "some college", 4 = "college graduate"), 
    + party identification (`partyid3` with levels 1= "democraets", 2 = "independents", 3 = "republicans", 4 = "apolitical" , and 
    + political ideology (`ideo` 1 = "liberal", 2 ="moderate", 3 = "conservative") 
```{r}
nes1992_clean = nes1992 %>% filter(year == 1992) %>% 
                            filter(!is.na(gender)) %>% 
                            filter(!is.na(race)) %>% 
                            filter(!is.na(educ1)) %>% 
                            filter(!is.na(partyid3)) %>% 
                            filter(!is.na(ideo)) 
                           
 
##factor gender
nes1992_clean$gender = factor(nes1992_clean$gender)
levels(nes1992_clean$gender) = list("male" = 1, "female" = 2)
##factor race
nes1992_clean$race = factor(nes1992_clean$race)
levels(nes1992_clean$race) = list("white" = 1, "black" = 2, "asian" = 3, "nativea merican" = 4, "hispanic" = 5)
##factor education
nes1992_clean$educ1 = factor(nes1992_clean$educ1)
levels(nes1992_clean$educ1) = list("no high school" = 1, "high school graduate" = 2, "some college" = 3, "college graduate" = 4)
##factor party identification
nes1992_clean$partyid3 = factor(nes1992_clean$partyid3)
levels(nes1992_clean$partyid3) = list("democraets" = 1, "independents" = 2, "republicans" = 3)
##political ideology
nes1992_clean$ideo = factor(nes1992_clean$ideo)
levels(nes1992_clean$ideo) = list("liberal" =1 , "moderate" = 3, "conservative" = 5)
index = which(is.na(nes1992_clean$partyid3))
nes1992_clean = nes1992_clean[-index,]
dim(nes1992_clean)
```

12.  Fit a logistic regression model predicting support for Bush given the the variables above and income as predictors and also consider interactions among the predictors. 
```{r}
##baseline model which doesn't have any interactions
glm_1992_ori = glm(vote~ income + gender + race + educ1 + partyid3 + ideo,
                          data = nes1992_clean,
                          family = binomial(link = "logit"))
summary(glm_1992_ori)
plot(glm_1992_ori)
```

```{r}
##add interaction
glm_1992_inr = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + race:gender + gender:partyid3, 
                           data = nes1992_clean, 
                           family = binomial(link = "logit"))
summary(glm_1992_inr)
plot(glm_1992_inr)
```

13.  Plot binned residuals using the function `binnedplot` from package `arm` versus some of the additional predictors in the 1992 dataframe.  Are there any suggestions that the mean or distribution of residuals is different across the levels of the other predictors and that they should be added to the model?  (Provide plots and any other summaries to explain).   
```{r}
##log odds
pred = predict(glm_1992_inr,type = "link")
##deviance residuals: signed square roots of the ith observation to the overall deviance
res = resid(glm_1992_inr)
binnedplot(pred, res)

##binned plot residual vs age
age_flat = as.numeric(data.matrix(nes1992_clean["age"]))
binnedplot(age_flat, res, xlab = "age")

##binned plot residual vs state
state_flat = as.numeric(data.matrix(nes1992_clean["state"]))
binnedplot(state_flat, res, xlab = "state")

##binned plot residual vs rlikes
rlikes_flat = as.numeric(data.matrix(nes1992_clean["rlikes"]))
binnedplot(rlikes_flat, res, xlab = "rlikes")
##binned plot residual vs dlikes
dlikes_flat = as.numeric(data.matrix(nes1992_clean["dlikes"]))
binnedplot(dlikes_flat, res, xlab = "dlikes")



```

From the binned plots above, it appears that the last two binned plots: residual vs dlikes and residual vs rlikes, the distribution of mean have some pattern across different levels in x axis. 
1. residual vs rlikes: It appears to have a increasing trend as rlikes increases.
2. residual vs dlikes: It appears to have a decreasing trend as dlikes increases.
Therefore, we tried to add rlikes and dlikes to the model. The following is the model and summary.It appears that both rlikes and dlikes are very significant in the model as the p-values are much less than 0.05

```{r}
glm_1992_new = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + race:gender + gender:partyid3+dlikes+rlikes, data = nes1992_clean, 
                           family = binomial(link = "logit"),
                           maxit = 100)
summary(glm_1992_new)
plot(glm_1992_new)
```


14.  Evaluate and compare the different models you fit.  Consider coefficient estimates (are they stable across models) and standard errors (any indications of identifiability problems), residual plots and deviances.
```{r}
anova(glm_1992_ori,glm_1992_inr, glm_1992_new,test = "Chisq")
```
 

15.  Compute the error rate of your model (see GH page 99) and compare it to the error rate of the null model.  We can define a function for the error rate as:
```{r error.rate, include=FALSE}
error.rate = function(pred, true) {
  mean((pred > .5 & true == 0) | (pred < .5 & true == 1))
}
```
```{r}
true = nes1992_clean$vote
pred = predict(glm_1992_new,type = "response")
error.rate(pred, true)
```

16.  For your chosen model, discuss and compare the importance of each input variable in the prediction.   Provide a neatly formatted table of odds ratios  and 95\% confidence intervals.
```{r}
glm_1992_idf = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + ideo_feel + race:gender + gender:partyid3, 
                           data = nes1992_clean, 
                           family = binomial(link = "logit"),
                           maxit = 100)

summary(glm_1992_idf)


ci = exp(cbind(coef(glm_1992_idf),confint(glm_1992_idf)))
suppressMessages(print(ci))
colnames(ci) = c("RR", "2.5", "97.5")
print(xtable(round(ci, 4)), comment=F)
```

17.  Provide a paragraph summarizing your findings and interpreting key coefficients (providing ranges of supporting values from above) in terms of the odds of voting for Bush.  Attempt to write this at a level that readers of the New York Times Upshot column could understand.   
```{r}
sum(is.na(nes["presvote"]))
```

18.  In the above analysis, we removed missing data.  Repeat the data cleaning steps, but remove only the rows where the response variable, `presvote` is missing.  Recode all of the predictors (including income) so that there is a level that is 'missing' for any NA's for each variable.  How many observations are there now compared to the complete data?






11.  Return to the 1992 year data. Filter out rows of `nes1992` with NA's in the variables below and  recode as factors using the levels in parentheses:
    + gender (1 = "male", 2 = "female"), 
    + race (1 = "white", 2 = "black", 3 = "asian", 4 = "native american", 5 = "hispanic", 7 = "other"), 
    + education ( use `educ1` with levels 1 = "no high school", 2 = "high school graduate", 3 = "some college", 4 = "college graduate"), 
    + party identification (`partyid3` with levels 1= "democraets", 2 = "independents", 3 = "republicans", 4 = "apolitical" , and 
    + political ideology (`ideo` 1 = "liberal", 2 ="moderate", 3 = "conservative") 
```{r}
# Data cleaning
# remove NA's for key variables first
nes1992_na = nes %>% filter(presvote %in% 1:2) %>% 
                     filter(year == 1992) %>%
                     mutate(female = gender -1,
                            black=race ==2,
                            vote = presvote ==2)



##factor gender
nes1992_na$gender = factor(nes1992_na$gender)
levels(nes1992_na$gender) = list("male" = 1, "female" = 2)
##factor race
nes1992_na$race = factor(nes1992_clean$race)
levels(nes1992_clean$race) = list("white" = 1, "black" = 2, "asian" = 3, "nativea merican" = 4, "hispanic" = 5)
##factor education
nes1992_clean$educ1 = factor(nes1992_clean$educ1)
levels(nes1992_clean$educ1) = list("no high school" = 1, "high school graduate" = 2, "some college" = 3, "college graduate" = 4)
##factor party identification
nes1992_clean$partyid3 = factor(nes1992_clean$partyid3)
levels(nes1992_clean$partyid3) = list("democraets" = 1, "independents" = 2, "republicans" = 3)
##political ideology
nes1992_clean$ideo = factor(nes1992_clean$ideo)
levels(nes1992_clean$ideo) = list("liberal" =1 , "moderate" = 3, "conservative" = 5)
```

19. For any of above variables, suggest possible reasons why they may be missing.
```{r}



```

20.  Rerun your selected model and create a table of parameter estimates and confidence intervals for the odds ratios.  You should have an additional coefficient for any categorical variable with missing data.   Comment on any changes in results for the model including the missing data and the previous one that used only complete data.
```{r}

```
