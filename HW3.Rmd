---
title: "HW3 Team 12"
author: "Thomas Fleming, Blaire Li, Marc Ryser, Hengqian Zhang"
date: "Due  February 10, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arm)
library(foreign)
library(magrittr)
library(dplyr)
library(ggplot2)
library(xtable)
library(knitr)
# add other libraries

```


We will explore logistic regression with the National Election Study data from Gelman & Hill (GH).  (See Chapter 4.7 for descriptions of some of the variables and 5.1 of GH for initial model fitting). 

[*The following code will read in the data and perform some filtering/recoding. Remove this text and modify the  code chunk options so that the code does not appear in the output.*]

```{r}
# Data are at http://www.stat.columbia.edu/~gelman/arm/examples/nes

nes <- read.dta("nes5200_processed_voters_realideo.dta",
                   convert.factors=F)
# Data cleaning
# remove NA's for key variables first
nes1992 = nes %>% filter(!is.na(black)) %>%
              filter(!is.na(female)) %>%
              filter(!is.na(educ1)) %>%
              filter(!is.na(age)) %>%
              filter(!is.na(state)) %>%
              filter(!is.na(income)) %>%
              filter(presvote %in% 1:2) %>% 
# limit to year 1992 to 2000 and add new variables
              filter(year == 1992) %>%
              mutate(female = gender -1,
                     black=race ==2,
                     vote = presvote ==2)
```

1. Summarize the data for 1992 noting which variables have missing data.  Which variables are categorical but are coded as numerically? 

ANSWER: In 1992, information in the form of 57 data fields was gathered from 1179 individuals (5 data fields specify survey details). Several fields have missing data values. The list below shows the names and numbers of missing entries (the data fields without missing entries are not listed). All variables are coded as numerical (except for 'black' and 'vote' which are logicals). However, most variables are categorical, with the exception of the following: the survey weights 'weight1', 'weight2', 'weight3', 'age', 'age_sq' (which is the age variable squared), 'ideo_feel', 'age_10' (=age/10) and 'age_sq_10' (=age^/10).

```{r, results='asis', echo=T}
#attach(nes1992)
a<-(apply(is.na(nes1992),2,sum))
print(a[a>0])
```

2. Fit the logistic regression to estimate the probability that an individual would vote Bush (Republican) as a function of income and provide a summary of the model.

ANSWER: First, we use the 'glm' function to perform a logistic regression of vote onto income.

```{r, }
vote.glm1 <-glm(vote ~ income, data=nes1992, family = binomial(link = "logit"))
summary(vote.glm1)
```

Based on this, we can estimate (point estimate) the probability that people of different income (1,2,3,4 or 5) vote for Bush. Below are the probabilities for increasing income levels

```{r}
inc<-1:5
w<-exp(vote.glm1$coefficients[1]+vote.glm1$coefficients[2]*inc)
v<-round(w/(1+w),3)
names(v)<-c("inc=1", "inc=2",  "inc=3", "inc=4", "inc=5")
print(v)
```


3. Obtain a point estimate and create a 95% confidence interval for the odds ratio for voting Republican for a rich person (income category 5) compared to a poor person (income category 1). *Hint this is more than a one unit change; calculate manually and then show how to modify the output from confint*. Provide a sentence interpreting the result.

ANSWER: We have that the odds ratio is equal to $e^{\beta_1 (5-1)}= e^{4\beta_1}$n (the intercept cancels out in the ratio of odds). Hence, we compute the confidence interval for $\beta_1$ and obtain the interval for the odds ratio as follows

```{r}
a<-confint(vote.glm1)
cf = exp(4*coef(vote.glm1)[2])
or.conf = exp(4*a[2,])
t = cbind(cf, t(or.conf))
suppressMessages(print(t))
```

With 95\% confidence, the odds for a rich person (income=5) to vote Republican (relative to the odds for a poor person (inc=1) to vote Republican) are contained in the interval $[2.37, 5.78]$.


4.  Obtain fitted probabilities and 95% confidence intervals for the income categories using the `predict` function.  Use `ggplot` to recreate the plots in figure 5.1 of Gelman & Hill.  *write a general function?*

ANSWER: First, use the 'preidict' function to extract the fitted probabilities and the standard errors. Then we use normal theory to obtain the 95\% CI. 

```{r}
new <- data.frame(income = c(1,2,3,4,5))
glm_fit<-predict(vote.glm1, newdata = new,  type="response" ,se.fit = TRUE)
inc<-c(1,2,3,4,5)
f<-data.frame(Income=inc, Prob=round(glm_fit$fit,3), CI.low=round(glm_fit$fit-1.96*glm_fit$se.fit,3),CI.up=round(glm_fit$fit+1.96*glm_fit$se.fit,3))
print(f, comment=F)

```

Next, we reproduce the two plots in Figure 5.1 in Gelman and Hill. For the second plot, we consulted their code to understand what they did: simulate 20 model coefficients based on the sampling distributions and create the corresponding plots. We repeated this procedure and used ggplot2 to make the corresponding plots.

```{r, echo=T}


vote.glm1 <-glm(vote ~ income, data=nes1992, family = binomial(link = "logit"))

# Figure 5.1(a)
x<-seq(from=-4, to=10, by=.5)
M<-data.frame(xx=x, prob.smooth=invlogit(coef(vote.glm1)[1]+coef(vote.glm1)[2]*x))
p1<-ggplot(M, aes(x=xx, y=prob.smooth))
fig1<-p1+geom_line(aes(y = prob.smooth))+geom_point(data=nes1992, aes(x=income, y=1*vote),size=.25,position = position_jitter(width = 0.25, height = .02))+geom_line(data=subset(M, M$xx>=1 & M$xx<=5), aes(x=xx, y=prob.smooth), size=2)+labs(x="Income", y="Pr(Rebublican vote)")+scale_x_continuous(breaks=c(1,2,3,4,5))

# Figure 5.1(b)
sims<-sim(vote.glm1)
x<-seq(from=1, to=5, by=.05)
M<-data.frame(xx=x, prob.smooth=invlogit(coef(vote.glm1)[1]+coef(vote.glm1)[2]*x))
fig2<-ggplot(M, aes(x=xx, y=prob.smooth))+geom_line(aes(y = prob.smooth))+geom_point(data=nes1992, aes(x=income, y=1*vote),size=.25,position = position_jitter(width = 0.25, height = .02))+labs(x="Income", y="Pr(Rebublican vote)")+scale_x_continuous(breaks=c(1,2,3,4,5))
for(q in 1:20){
  M2<-data.frame(xx=x, prob.smooth=invlogit(sims@coef[q,1]+sims@coef[q,2]*x))

  fig2<-fig2+geom_line(data=M2,aes(y = prob.smooth), size=.25, linetype=3)
}

par(mfrow=c(2,2)) 
plot(fig1)

plot(fig2)

```



5.  What does the residual deviance or any diagnostic plots suggest about the model?  (provide code for p-values and output and plots) 

ANSWER: First we compute the residual deviance, defined as the change in deviance between the fitted model and the saturated model, and use the $\chi^2$ statistic to check whether the model is satisfactorily.

```{r}
print(vote.glm1$deviance)
pchisq(vote.glm1$deviance, vote.glm1$df.residual, lower.tail = F)

```

The very high residual deviance (and corresponding small p-value) implies that the model is not satisfactory. Next, we consider the summary plots

```{r}
par(mfrow=c(2,2)) 
plot(vote.glm1)

```
We need to proceed with caution in interpreting these plots. The residuals vs fitted pot does not look alarming per se, the Q-Q plot is off the line for normality, but this doesn't mean there is an issue a priori (there are not very heavy tails).  Heteroscidacity is to be expected in a logistic regression model. The leverage plot does not reveal any large leverage points, and the Pearson residuals are not excessively large. None of the points exceeds Cook's distance of 0.5 which is reassuring. 


6. Create a new data set by the filtering and mutate steps above, but now include years between 1952 and 2000.

```{r}
nes52.2000 = nes %>% filter(!is.na(black)) %>%
              filter(!is.na(female)) %>%
              filter(!is.na(educ1)) %>%
              filter(!is.na(age)) %>%
              filter(!is.na(state)) %>%
              filter(!is.na(income)) %>%
              filter(presvote %in% 1:2) %>% 
              filter(year >= 1952 & year <= 2000) %>%
              mutate(female = gender -1,
                     black=race ==2,
                     vote = presvote == 2)
```


7. Fit a separate logistic regression for each year from 1952 to 2000, using the `subset` option in `glm`,  i.e. add `subset=year==1952`.  For each find the 95% Confidence interval for the odds ratio of voting republican for rich compared to poor for each year in the data set from 1952 to 2000.

```{r}

yr <- unique(nes52.2000$year)
OR<-data.frame(year=yr, est=numeric(length(yr)), CI.lwr=numeric(length(yr)),CI.upr=numeric(length(yr)) )
inc.indiv<-rep(0,length(yr)) # used for question 9
intercept.store<-rep(0,length(yr)) # used for question 9
for (i in 1:length(yr)) {
rep <- glm(vote ~ income, data=nes52.2000, family=binomial(link="logit"), subset=year==yr[i])
OR$est[i]<-exp(4*rep$coefficients[2])
rep.ci <- suppressMessages(confint(rep))
OR[i,3:4] <- exp(4*rep.ci[2,])
inc.indiv[i]<-rep$coefficients[2]
intercept.store[i]<-rep$coefficients[1]
}
print(round(OR,2))
```





8.  Using `ggplot` plot the confidence intervals over time similar to the display in Figure 5.4.

```{r}
# use ggplot2
p<-ggplot(OR, aes(x=year, y=est)) +geom_point()+  geom_errorbar(aes(ymin=CI.lwr, ymax=CI.upr), width=.1)
p<-p+xlab("Year") + ylab("Odds ratio (95%-CI)")+ylim(0,10) + geom_hline(yintercept=c(1), linetype="dotted")
plot(p)
```

9. Fit a logistic regression using income and year as a factor  with an interaction i.e. `income*factor(year)` to the data from 1952-2000.  Find the log odds ratio for income for each year by combining parameter estimates and show that these are the same as in the respective individual logistic regression models fit separately to the data for each year.

```{r}
inc.int <- rep(0,length(unique(nes52.2000$year)))
rep_interact <- glm(vote ~ income*factor(year), data = nes52.2000, family=binomial(link="logit"))
for (i in 1:length(yr)) {
if (i==1) {
  inc.i <- rep_interact$coefficients[2]
} else {
inc.i <- rep_interact$coefficients[2] + rep_interact$coefficients[i + 13]
}
inc.int[i]<-inc.i;
}
cbind(inc.int,inc.indiv)
```
As shown above, the log odds ratios for income are exactly the same generated from both processes (the individual regression coefficients were generated in #7).

10.  Create a plot of fitted probabilities and confidence intervals as in question 4, with curves for all years in the same plot.

```{r}
x<-seq(from=1, to=5, by=.05)

M<-data.frame(xx=x, prob.smooth=invlogit(intercept.store[1]+inc.indiv[1]*x))

fig2<-ggplot(M, aes(x=xx, y=prob.smooth), color = year)+geom_line(aes(y = prob.smooth))+geom_point(data=nes52.2000, aes(x=income, y=1*vote), color="hotpink", size=.25, position = position_jitter(width = 0.25, height = .1))+labs(x="Income", y="Pr(Republican vote)")

rep <- glm(vote ~ income, data=nes52.2000, family=binomial(link="logit"), subset=year==yr[1])
rep.sim <- sim(rep)

for(i in 2:length(yr)){
  M2<-data.frame(xx=x, prob.smooth=invlogit(intercept.store[i]+inc.indiv[i]*x))

  fig2<-fig2+geom_line(data=M2,aes(y = prob.smooth), color=i, size=.25)
  
  for(j in 1:20) {
  M3<-data.frame(xx=x, prob.smooth=invlogit(rep.sim@coef[j,1]+rep.sim@coef[j,2]*x))

  fig2<-fig2+geom_line(data=M3,aes(y = prob.smooth), color=i-1, size=.25, linetype=3)
  }
  
  rep <- glm(vote ~ income, data=nes52.2000, family=binomial(link="logit"), subset=year==yr[i])
  rep.sim <- sim(rep)
}

for(k in 1:20) {
  M3<-data.frame(xx=x, prob.smooth=invlogit(rep.sim@coef[k,1]+rep.sim@coef[k,2]*x))

  fig2<-fig2+geom_line(data=M3,aes(y = prob.smooth), color=13, size=.25, linetype=3)
}
fig2<-fig2+scale_fill_discrete(name = "Years", labels=c(yr[1], yr[2], yr[3], yr[4], yr[5], yr[6], yr[7], yr[8], yr[9], yr[10], yr[11], yr[12], yr[13]))

par(mfrow=c(2,2)) 

plot(fig2)
```
*Used code from Gelman & Hill Figure 5.1 as guidance.

11.  Return to the 1992 year data. Filter out rows of `nes1992` with NA's in the variables below and  recode as factors using the levels in parentheses:
    + gender (1 = "male", 2 = "female"), 
    + race (1 = "white", 2 = "black", 3 = "asian", 4 = "native american", 5 = "hispanic", 7 = "other"), 
    + education ( use `educ1` with levels 1 = "no high school", 2 = "high school graduate", 3 = "some college", 4 = "college graduate"), 
    + party identification (`partyid3` with levels 1= "democraets", 2 = "independents", 3 = "republicans", 4 = "apolitical" , and 
    + political ideology (`ideo` 1 = "liberal", 2 ="moderate", 3 = "conservative") 
```{r}
nes1992_clean = nes1992 %>% filter(year == 1992) %>% 
                            filter(!is.na(gender)) %>% 
                            filter(!is.na(race)) %>% 
                            filter(!is.na(educ1)) %>% 
                            filter(!is.na(partyid3)) %>% 
                            filter(!is.na(ideo)) 
                           
 
##factor gender
nes1992_clean$gender = factor(nes1992_clean$gender)
levels(nes1992_clean$gender) = list("male" = 1, "female" = 2)
##factor race
nes1992_clean$race = factor(nes1992_clean$race)
levels(nes1992_clean$race) = list("white" = 1, "black" = 2, "asian" = 3, "nativea merican" = 4, "hispanic" = 5)
##factor education
nes1992_clean$educ1 = factor(nes1992_clean$educ1)
levels(nes1992_clean$educ1) = list("no high school" = 1, "high school graduate" = 2, "some college" = 3, "college graduate" = 4)
##factor party identification
nes1992_clean$partyid3 = factor(nes1992_clean$partyid3)
levels(nes1992_clean$partyid3) = list("democraets" = 1, "independents" = 2, "republicans" = 3)
##political ideology
nes1992_clean$ideo = factor(nes1992_clean$ideo)
levels(nes1992_clean$ideo) = list("liberal" =1 , "moderate" = 3, "conservative" = 5)
index = which(is.na(nes1992_clean$partyid3))
nes1992_clean = nes1992_clean[-index,]

```

12.  Fit a logistic regression model predicting support for Bush given the the variables above and income as predictors and also consider interactions among the predictors. 
```{r}
##baseline model which doesn't have any interactions
glm_1992_ori = glm(vote~ income + gender + race + educ1 + partyid3 + ideo,
                          data = nes1992_clean,
                          family = binomial(link = "logit")
                           )

summary(glm_1992_ori)
plot(glm_1992_ori)
```

```{r}
##add interaction
glm_1992_inr = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + race:gender + gender:partyid3, 
                           data = nes1992_clean, 
                           family = binomial(link = "logit"))
summary(glm_1992_inr)
plot(glm_1992_inr)
```

13.  Plot binned residuals using the function `binnedplot` from package `arm` versus some of the additional predictors in the 1992 dataframe.  Are there any suggestions that the mean or distribution of residuals is different across the levels of the other predictors and that they should be added to the model?  (Provide plots and any other summaries to explain).   
```{r}
##log odds
pred = predict(glm_1992_inr,type = "link")
##deviance residuals: signed square roots of the ith observation to the overall deviance
res = resid(glm_1992_inr)
binnedplot(pred, res)

##binned plot residual vs age
age_flat = as.numeric(data.matrix(nes1992_clean["age"]))
binnedplot(age_flat, res, xlab = "age")

##binned plot residual vs state
state_flat = as.numeric(data.matrix(nes1992_clean["state"]))
binnedplot(state_flat, res, xlab = "state")

##binned plot residual vs rlikes
rlikes_flat = as.numeric(data.matrix(nes1992_clean["rlikes"]))
binnedplot(rlikes_flat, res, xlab = "rlikes")
##binned plot residual vs dlikes
dlikes_flat = as.numeric(data.matrix(nes1992_clean["dlikes"]))
binnedplot(dlikes_flat, res, xlab = "dlikes")
```

From the binned plots above, it appears that the last two binned plots: residual vs dlikes and residual vs rlikes, the distribution of mean have some pattern across different levels in x axis. 
1. residual vs rlikes: It appears to have a increasing trend as rlikes increases.
2. residual vs dlikes: It appears to have a decreasing trend as dlikes increases.
Therefore, we tried to add rlikes and dlikes to the model. The following is the model and summary.It appears that both rlikes and dlikes are very significant in the model as the p-values are much less than 0.05

```{r}
glm_1992_new = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + race:gender + gender:partyid3+dlikes+rlikes, data = nes1992_clean, 
                           family = binomial(link = "logit"))
summary(glm_1992_new)
plot(glm_1992_new)
```


14.  Evaluate and compare the different models you fit.  Consider coefficient estimates (are they stable across models) and standard errors (any indications of identifiability problems), residual plots and deviances.
```{r}
summary(glm_1992_ori)
summary(glm_1992_inr)
summary(glm_1992_new)
anova(glm_1992_ori,glm_1992_inr, glm_1992_new,test = "Chisq")


```
 Comparing the coefficents and standard errors, we find that the coefficients are relatively stable. However, there seems to be a separation(identifiability) problem because the standard error and coef for genderfemale:raceasian are unusually large.
 
Among the three models, glm_1992_new has the lowest residual deviance 320.01, which is much smaller compared to other models.

Therefore, the model glm_1992_new seems to be the best choice for now.

15.  Compute the error rate of your model (see GH page 99) and compare it to the error rate of the null model.  We can define a function for the error rate as:
```{r error.rate, include=FALSE}
error.rate = function(pred, true) {
  mean((pred > .5 & true == 0) | (pred < .5 & true == 1))
}
```

```{r}
#compute error rate for glm_1992_new
true = nes1992_clean$vote
pred = predict(glm_1992_new,type = "response")
error.rate(pred, true)
#compute error rate for glm_1992_ori
pred = predict(glm_1992_ori,type = "response")
error.rate(pred, true)

#compute error rate for glm_1992_inr
pred = predict(glm_1992_inr,type = "response")
error.rate(pred, true)

#compute error rate for null model
null_model = glm(vote~1,data=nes1992_clean)
pred = predict(null_model,type = "response")
error.rate(pred, true)
```
Comparing the error rates, we can see that glm_1992_new provides the lowest error rate 0.058.


16.  For your chosen model, discuss and compare the importance of each input variable in the prediction.   Provide a neatly formatted table of odds ratios  and 95\% confidence intervals.
```{r}
glm_1992_new = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + race:gender + gender:partyid3+dlikes+rlikes, data = nes1992_clean, 
                           family = binomial(link = "logit")
                           )


ci = cbind(exp(glm_1992_new$coefficients),exp(confint.default(glm_1992_new)))
colnames(ci) = c("RR", "2.5", "97.5")
suppressMessages(print(ci))

x = which(nes1992_clean["gender"] == "female")
y = which(nes1992_clean["race"]=="asian")

intersect(x, y)
nes1992_clean$vote[intersect(x, y)]
```
In general, the bigger the odds ratio is, the more importance the corresponding variable will be. In our model, partyid3 republicans, partyid3independents,ideoconservative and genderfemale:racehispanic  are important variable. One extreme case  
is genderfemale:raceasian. Based on our analysis, the intersects between female category and asian category are only 5 observation who all vote for bush. In this case, it is perfectly classified by it and we need consider constrained logistic regression.  

17.  Provide a paragraph summarizing your findings and interpreting key coefficients (providing ranges of supporting values from above) in terms of the odds of voting for Bush.  Attempt to write this at a level that readers of the New York Times Upshot column could understand. 

  First, gender in our model is significant. Women are more likely to vote for Bush than man. Also, party identification matters in our model. Republicans and Independents are more likely to vote for bush than democrats. Also, people whose political ideology is conservative are more likely to vote for bush than people who have other political ideology. Also, people who feel good with Democratic Party are less likely to vote bush and people who feel good with Republican Party are more likely to vote bush.
  From our model, there is no evidence that people with high education have the trend to vote for bush or not.


18.  In the above analysis, we removed missing data.  Repeat the data cleaning steps, but remove only the rows where the response variable, `presvote` is missing.  Recode all of the predictors (including income) so that there is a level that is 'missing' for any NA's for each variable.  How many observations are there now compared to the complete data?


```{r}
# Data cleaning
# remove NA's for key variables first
nes1992_na = nes %>% filter(presvote %in% 1:2) %>% 
                     filter(year == 1992) %>%
                     mutate(female = gender -1,
                            black=race ==2,
                            vote = presvote ==2)

##remove 9 and na in partyid3 columns
pid3_9ix = which(nes1992_na["partyid3"] == 9)
pid3_naix = which(is.na(nes1992_na$partyid3) == TRUE)



nes1992_na = nes1992_na[-c(pid3_9ix,pid3_naix)]

##factor gender
nes1992_na$gender = factor(nes1992_na$gender)
levels(nes1992_na$gender) = list("male" = 1, "female" = 2)

##factor race
nes1992_na$race[which(is.na(nes1992_na$race) == TRUE)] = "na"
nes1992_na$race = factor(nes1992_na$race)
levels(nes1992_na$race) = list("white" = "1", "black" = "2", "asian" = "3", "nativea merican" = "4", "hispanic" = "5", "na" = "na")

##factor education
nes1992_na$educ1[which(is.na(nes1992_na$educ1) == TRUE)] = "na"
nes1992_na$educ1 = factor(nes1992_na$educ1)
levels(nes1992_na$educ1) = list("no high school" = "1", "high school graduate" = "2", "some college" = "3", "college graduate" = "4", "na" = "na")

##factor party identification
nes1992_na$partyid3[which(is.na(nes1992_na$partyid3) == TRUE)] = "na"
nes1992_na$partyid3 = factor(nes1992_na$partyid3)
levels(nes1992_na$partyid3) = list("democraets" = "1", "independents" = "2", "republicans" = "3")

##political ideology
nes1992_na$ideo[which(is.na(nes1992_na$ideo) == TRUE)] = "na"
nes1992_clean$ideo = factor(nes1992_clean$ideo)
levels(nes1992_clean$ideo) = list("liberal" ="1" , "moderate" = "3", "conservative" = "5", "na" = "na")

##income
nes1992_na$income[which(is.na(nes1992_na$income) == TRUE)] = "na"
nes1992_clean$income = factor(nes1992_clean$income)
levels(nes1992_clean$income) = list("1" = "1" , "2" = "2", "3" = "3", "4" = "4", "5" = "5", "na" = "na")


dim(nes1992)
dim(nes1992_na)
```
  Cleaning and recode NA procedure are given above. Here I deleted 9 and na in partyid3 column since there are only one 9 and 3nas in this columns. After deleting, I factorize each variable and recode its corresponding level.
  For the complete data, there are 1304 observations in total. For missing-value data, there are 1179 observations in total.


19. For any of above variables, suggest possible reasons why they may be missing.
  One of the reason for the missing values is that people do not want others know their real conditions. For example, there is a guy who tells his friends he supports Democratic and in fact he support republican. Anthoer example is that many people do not want to release their salaries.

 
20.  Rerun your selected model and create a table of parameter estimates and confidence intervals for the odds ratios.  You should have an additional coefficient for any categorical variable with missing data.   Comment on any changes in results for the model including the missing data and the previous one that used only complete data.
```{r}
##add interaction
glm_1992_new_na = glm(vote~  income + gender + race + educ1 + partyid3 + ideo + race:gender + gender:partyid3+dlikes+rlikes, data = nes1992_na, 
                           family = binomial(link = "logit"),
                           maxit = 100)

summary(glm_1992_new_na)

ci_na = exp(cbind(coef(glm_1992_new_na),confint.default(glm_1992_new_na)))
colnames(ci_na) = c("RR", "2.5", "97.5")
suppressMessages(print(ci_na))
```

We compared missing data model with non-missing data, there is no significant change in missing data model. The only interesting change is that one interaction term tends to be significant. Women who are republicans are less likely to vote for bush than women who are not republicans. The overall no change indicates that all missing values are randomly missing. There is no pattern for these missing values. 
